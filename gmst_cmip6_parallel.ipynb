{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.7/site-packages/ipykernel_launcher.py:16: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gcsfs\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import intake\n",
    "import xesmf as xe\n",
    "%matplotlib inline\n",
    "import cartopy\n",
    "from cartopy import util\n",
    "import cartopy.crs as ccrs\n",
    "import zarr\n",
    "import dask.array as da  \n",
    "from dask.diagnostics import ProgressBar\n",
    "from tqdm.autonotebook import tqdm\n",
    "import nc_time_axis\n",
    "import time\n",
    "\n",
    "import cmip6_preprocessing\n",
    "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter,\n",
    "                               AutoMinorLocator)\n",
    "\n",
    "from dask.distributed import Client, progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cat_url = \"/space/hall4/sitestore/eccc/crd/CMIP6/final/canesm_final.json\"\n",
    "cat_url = \"https://storage.googleapis.com/cmip6/pangeo-cmip6.json\"\n",
    "col = intake.open_esm_datastore(cat_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix some inconsistencies across datasets like dimension names, bounds, extra variables, etc.\n",
    "from cmip6_preprocessing.preprocessing import combined_preprocessing, replace_x_y_nominal_lat_lon, rename_cmip6\n",
    "\n",
    "def wrapper(ds):\n",
    "    ds = ds.copy()\n",
    "    #ds = rename_cmip6(ds)\n",
    "    #ds = replace_x_y_nominal_lat_lon(ds)\n",
    "    \n",
    "    if ('longitude' in ds.dims) and ('latitude' in ds.dims):\n",
    "        ds = ds.rename({'longitude':'lon', 'latitude': 'lat'}) # some models labelled dimensions differently...\n",
    "    if ('bnds' in ds.dims): \n",
    "        ds=ds.drop_dims('bnds')\n",
    "    if ('vertex' in ds.dims): \n",
    "        ds=ds.drop_dims('vertex')\n",
    "    if ('height' in ds.dims): \n",
    "        ds=ds.drop_dims('height')\n",
    "    if ('height' in ds): \n",
    "        ds=ds.drop_vars('height')\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "                \n",
      "--> There is/are 34 group(s)\n",
      "[########################################] | 100% Completed |  2.5s\n",
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "                \n",
      "--> There is/are 7 group(s)\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "                \n",
      "--> There is/are 23 group(s)\n",
      "[########################################] | 100% Completed |  1.5s\n",
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "                \n",
      "--> There is/are 22 group(s)\n",
      "[########################################] | 100% Completed |  1.3s\n",
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "                \n",
      "--> There is/are 22 group(s)\n",
      "[########################################] | 100% Completed |  1.3s\n",
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "                \n",
      "--> There is/are 22 group(s)\n",
      "[########################################] | 100% Completed |  1.4s\n"
     ]
    }
   ],
   "source": [
    "dset_dicts={}\n",
    "experiment_ids=['historical', 'ssp119','ssp126', 'ssp245', 'ssp370', 'ssp585']\n",
    "\n",
    "for exp in experiment_ids:\n",
    "    query = dict(table_id=['Amon'], \n",
    "                 variable_id=['tas'],\n",
    "              experiment_id=exp, member_id='r1i1p1f1')\n",
    "    cat = col.search(**query)\n",
    "    dset_dicts[exp] = cat.to_dataset_dict(zarr_kwargs={'consolidated': True}, \n",
    "                                        storage_options={'token': 'anon'}, \n",
    "                                        preprocess=wrapper,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_ids=['historical', 'ssp119','ssp126', 'ssp245', 'ssp370', 'ssp585']\n",
    "dset_dicts_proc = {}\n",
    "\n",
    "for exp in experiment_ids:\n",
    "    tmp = {}\n",
    "    for name, data in dset_dicts[exp].items():\n",
    "        model = name.split('.')[2]\n",
    "        tmp[model] = data\n",
    "    dset_dicts_proc[exp] = tmp\n",
    "\n",
    "dset_dicts_match = {}\n",
    "dset_dicts_match['historical'] = dset_dicts_proc['historical']\n",
    "    \n",
    "experiment_ids=['ssp119','ssp126', 'ssp245', 'ssp370', 'ssp585']\n",
    "for exp in experiment_ids:\n",
    "    tmp = {}\n",
    "    for name, data in dset_dicts_proc[exp].items():\n",
    "        if name in dset_dicts_proc['historical'].keys(): \n",
    "            tmp[name] = data\n",
    "    dset_dicts_match[exp] = tmp                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gmst(ds):\n",
    "    print(ds)\n",
    "    cos_lat_2d = np.cos(np.deg2rad(ds['lat'])) * xr.ones_like(ds['lon'])\n",
    "    gmst = ((ds * cos_lat_2d).sum(dim=['lon', 'lat'])/cos_lat_2d.sum(dim=['lat','lon'])).compute()\n",
    "    return gmst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:40553</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>6</li>\n",
       "  <li><b>Cores: </b>6</li>\n",
       "  <li><b>Memory: </b>21.04 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:40553' processes=6 threads=6, memory=21.04 GB>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Client(threads_per_worker=1, n_workers=6)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f93f622247534aada43792357b5f9fbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Worker process still alive after 3 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3 seconds, killing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.7 s, sys: 2.09 s, total: 24.7 s\n",
      "Wall time: 2min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "gmst_dicts = {}\n",
    "for exp, dic in tqdm(dset_dicts_match.items()):\n",
    "    # Use dask map to distribute this to client workers\n",
    "    results = client.map(calc_gmst, dic.values())\n",
    "    results = client.gather(results)\n",
    "    tmp = dict(zip(dic.keys(), results)) \n",
    "    gmst_dicts[exp] = tmp\n",
    "    \n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>&lt;xarray.Dataset&gt;\n",
       "Dimensions:    (member_id: 1, time: 1980)\n",
       "Coordinates:\n",
       "  * time       (time) object 1850-01-16 12:00:00 ... 2014-12-16 12:00:00\n",
       "  * member_id  (member_id) &lt;U8 &#x27;r1i1p1f1&#x27;\n",
       "Data variables:\n",
       "    tas        (member_id, time) float64 285.0 284.9 285.6 ... 288.4 287.6 286.9</pre>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:    (member_id: 1, time: 1980)\n",
       "Coordinates:\n",
       "  * time       (time) object 1850-01-16 12:00:00 ... 2014-12-16 12:00:00\n",
       "  * member_id  (member_id) <U8 'r1i1p1f1'\n",
       "Data variables:\n",
       "    tas        (member_id, time) float64 285.0 284.9 285.6 ... 288.4 287.6 286.9"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmst_dicts['historical']['CanESM5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "gmst_anom_dicts = {}\n",
    "#glob_area = areacella.sum(dim=['lon', 'lat'])\n",
    "for exp, dic in gmst_dicts.items():\n",
    "    tmp = {}\n",
    "    for model, data in dic.items():\n",
    "        try:\n",
    "            tmp[model] = (data.sel(time=slice('1850','2100')) - \n",
    "                          gmst_dicts['historical'][model].sel(time=slice('1850','1900')).mean(dim='time')\n",
    "                         ).groupby('time.year').mean(dim='time')#.compute()\n",
    "        except:\n",
    "            print('Failed:', model)\n",
    "    gmst_anom_dicts[exp] = tmp   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gmst_anoms = {}\n",
    "#glob_area = areacella.sum(dim=['lon', 'lat'])\n",
    "for exp, dic in gmst_anom_dicts.items():\n",
    "    dss = [ds for key, ds in dic.items()]\n",
    "    gmst_anoms[exp] = xr.concat(dss, dim='source_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download obs from my FTP.\n",
    "#! wget ftp://crd-data-donnees-rdc.ec.gc.ca/pub/CCCMA/nswart/fldmean_NASA-GISS_gistemp1200_GHCNv4_ERSSTv5.nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = xr.open_dataset('fldmean_NASA-GISS_gistemp1200_GHCNv4_ERSSTv5.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_anom = (obs - obs.sel(time=slice('1850','1900')).mean(dim='time')).groupby('time.year').mean(dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_bounds = {}\n",
    "lower_bounds = {}\n",
    "for key, data in gmst_anoms.items():\n",
    "    upper_bounds[key] = (data['tas'].mean(dim='source_id') + \n",
    "                         data['tas'].std(dim='source_id')*2.0).squeeze()#/np.sqrt(len(data['tas'].member_id)))\n",
    "    lower_bounds[key] = (data['tas'].mean(dim='source_id') -\n",
    "                         data['tas'].std(dim='source_id')*2.0).squeeze()#/np.sqrt(len(data['tas'].member_id))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fig, ax = plt.subplots(1,1,figsize=(6,4))\n",
    "\n",
    "colors = ['k', 'tab:cyan', 'tab:blue', 'darkorange', 'tab:red']\n",
    "label = ['historical', 'SSP1-26', 'SSP2-45', 'SSP3-70', 'SSP5-85']\n",
    "for i, key in enumerate(['historical', 'ssp126', 'ssp245', 'ssp370', 'ssp585']):\n",
    "    ax.fill_between(gmst_anoms[key].year, upper_bounds[key], lower_bounds[key], \n",
    "                    facecolor=colors[i], alpha=0.3)\n",
    "\n",
    "    ax.plot(gmst_anoms[key].year, gmst_anoms[key]['tas'].mean(dim='source_id'), \n",
    "            color=colors[i], label=label[i])\n",
    "    \n",
    "ax.plot(obs_anom.year, obs_anom.tempanomaly.squeeze(), 'tab:pink', label='NASA-GISS obs.')\n",
    "ax.legend(frameon=False, loc='upper left')\n",
    "ax.set_ylabel('GMST anomaly ($^{\\circ}$C)')\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_xlim([1850, 2100])\n",
    "ax.text(2020,-2.5, '@neil_c_swart / CCCma', alpha=0.5)\n",
    "\n",
    "ax.set_title('CMIP6 mean anomalies relative to 1850-1900')\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(0.5))\n",
    "ax.tick_params(axis='both', left=True, right=True, which='both', direction='in')\n",
    "fig.savefig('CMIP6_GMST_anomalies_plus-obs.png', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
